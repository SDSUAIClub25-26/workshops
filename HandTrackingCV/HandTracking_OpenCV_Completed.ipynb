{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDJW1BSOB9ZE"
   },
   "source": [
    "### Hand Tracking with OpenCV and Mediapipe!\n",
    "\n",
    "**Steps to Get Started:** (Also reference README within this folder)\n",
    "1. Clone this repository\n",
    "2. Ensure you have Python 3.11.9 installed on your computer (we will go through how to set it up)\n",
    "3. Create a virtual environment using this python version\n",
    "4. pip install requirements.txt (ensures no issues with different versions)\n",
    "5. Run the first code block with imports to ensure it works properly\n",
    "5. Once you've installed the necessary packages, you're ready to begin!\n",
    "\n",
    "Helpful Article:\n",
    "https://medium.com/@Mert.A/how-to-create-a-finger-counter-with-python-and-mediapipe-cc6c3911ad09\n",
    "\n",
    "Helpful Video:\n",
    "https://www.youtube.com/watch?v=RRBXVu5UE-U&t=78s\n",
    "\n",
    "Docs:\n",
    "https://mediapipe.readthedocs.io/en/latest/solutions/hands.html#:~:text=%7C-,ML%20Pipeline,a%20dedicated%20hand%20renderer%20subgraph.\n",
    "\n",
    "Setting Up Python in VS Code:\n",
    "https://code.visualstudio.com/docs/python/python-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NH6Kyw1IByMR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pyautogui\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(\"Everything imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sVn2EyAlByCc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1758150005.461967 1013049 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2 Pro\n"
     ]
    }
   ],
   "source": [
    "mpHands = mp.solutions.hands\n",
    "hands = mpHands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
    "mpDraw = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbXGynokB3yy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "s = pyautogui.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RoloM2K5BLuy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1758150005.477368 1015784 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1758150005.485516 1015790 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "2025-09-17 16:00:07.337 python[89606:1013049] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-09-17 16:00:07.337 python[89606:1013049] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture a frame from the webcam\n",
    "    # ret is a boolean of whether frame was read, frame is numpy array in BGR format\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip the frame horizontally for a mirror effect\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Display the frame in a window titled 'Output'\n",
    "    cv2.imshow('Output', frame)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed in the OpenCV window\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close any OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1) # delay of 1 second each time it reads a key press"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ksm0t04TBcSG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hand tracking\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # capture webcame frame and shape (width and height)\n",
    "    ret, frame = cap.read()\n",
    "    # defining the frame\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    # -------- ADD THIS CODE NEXT ---------------\n",
    "    # have mediapipe hands predict hand landmarks\n",
    "    x, y, c = frame.shape\n",
    "    result = hands.process(frame)\n",
    "    # iterate through the predicted landmarks adjusting them to the window, and\n",
    "    # and outputting them to the opencv window\n",
    "    if result.multi_hand_landmarks:\n",
    "        landmarks = []\n",
    "        for handslms in result.multi_hand_landmarks:\n",
    "            for lm in handslms.landmark:\n",
    "                lmx = int(lm.x * x)\n",
    "                lmy = int(lm.y * y)\n",
    "                landmarks.append([lmx, lmy])\n",
    "            mpDraw.draw_landmarks(frame, handslms, mpHands.HAND_CONNECTIONS)\n",
    "    # ------- FINISHED ADDING NEW CODE ----------\n",
    "    cv2.imshow('Output', frame)\n",
    "    # if q is pressed, program exits\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OORJWznsB8mt"
   },
   "source": [
    "<img src=\"hand_tracking_landmarks.webp\" alt=\"Hand Tracker Visualization with Mediapipe\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "k4Hu-vEIBkw1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Moving Cursor\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # capture webcame frame and shape (width and height)\n",
    "    ret, frame = cap.read()\n",
    "    # defining the frame\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    x, y, c = frame.shape\n",
    "    # have mediapipe hands predict hand landmarks\n",
    "    result = hands.process(frame)\n",
    "    # iterate through the predicted landmarks adjusting them to the window, and\n",
    "    # and outputting them to the opencv window\n",
    "    if result.multi_hand_landmarks:\n",
    "        landmarks = []\n",
    "        for handslms in result.multi_hand_landmarks:\n",
    "            # take the 8th landmark (index finger point) and move the cursor to that landmarks x and y value\n",
    "            # ADD THE LINE BELOW NEXT\n",
    "            index_x = int(handslms.landmark[8].x * s[0])\n",
    "            index_y = int(handslms.landmark[8].y * s[1])\n",
    "            pyautogui.moveTo(index_x, index_y, _pause=False)\n",
    "            for lm in handslms.landmark:\n",
    "                lmx = int(lm.x * x)\n",
    "                lmy = int(lm.y * y)\n",
    "                landmarks.append([lmx, lmy])\n",
    "            mpDraw.draw_landmarks(frame, handslms, mpHands.HAND_CONNECTIONS)\n",
    "\n",
    "    cv2.imshow('Output', frame)\n",
    "    # if q is pressed, program exits\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xMMQnEgtBliQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1758150009.798319 1015780 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'index_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m thumb_x = \u001b[38;5;28mint\u001b[39m(handslms.landmark[\u001b[32m4\u001b[39m].x * s[\u001b[32m0\u001b[39m])\n\u001b[32m     25\u001b[39m thumb_y = \u001b[38;5;28mint\u001b[39m(handslms.landmark[\u001b[32m4\u001b[39m].y * s[\u001b[32m1\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m distance = ((\u001b[43mindex_x\u001b[49m - thumb_x) ** \u001b[32m2\u001b[39m + (index_y - thumb_y) ** \u001b[32m2\u001b[39m) ** \u001b[32m0.5\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# If distance is small enough, simulate a click\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m distance < \u001b[32m40\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m time.time() - last_click > \u001b[32m0.5\u001b[39m:\n",
      "\u001b[31mNameError\u001b[39m: name 'index_x' is not defined"
     ]
    }
   ],
   "source": [
    "#Clicking\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "# -------- ADD THIS VARIABLE ---------------\n",
    "last_click = 0\n",
    "while True:\n",
    "    # capture webcame frame and shape (width and height)\n",
    "    ret, frame = cap.read()\n",
    "    x, y, c = frame.shape\n",
    "    # defining the frame\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    # have mediapipe hands predict hand landmarks\n",
    "    result = hands.process(frame)\n",
    "    # iterate through the predicted landmarks adjusting them to the window, and\n",
    "    # and outputting them to the opencv window\n",
    "    if result.multi_hand_landmarks:\n",
    "        landmarks = []\n",
    "        for handslms in result.multi_hand_landmarks:\n",
    "            # take the 8th landmark (index finger point) and move the cursor to that landmarks x and y value\n",
    "            pyautogui.moveTo(int(handslms.landmark[8].x * s[0]), int(handslms.landmark[8].y * s[1]), _pause=False)\n",
    "\n",
    "            # -------- ADD THIS CODE NEXT ---------------\n",
    "            # Detect click gesture by calculating distance between thumb (landmark 4) and index (landmark 8)\n",
    "            thumb_x = int(handslms.landmark[4].x * s[0])\n",
    "            thumb_y = int(handslms.landmark[4].y * s[1])\n",
    "            distance = ((index_x - thumb_x) ** 2 + (index_y - thumb_y) ** 2) ** 0.5\n",
    "\n",
    "            # If distance is small enough, simulate a click\n",
    "            if distance < 40 and time.time() - last_click > 0.5:\n",
    "                pyautogui.click()\n",
    "                last_click = time.time()\n",
    "            # ------- FINISHED ADDING NEW CODE ----------\n",
    "\n",
    "            # for lm in handslms.landmark:\n",
    "            #     lmx = int(lm.x * x)\n",
    "            #     lmy = int(lm.y * y)\n",
    "            #     landmarks.append([lmx, lmy])\n",
    "            mpDraw.draw_landmarks(frame, handslms, mpHands.HAND_CONNECTIONS)\n",
    "    cv2.imshow('Output', frame)\n",
    "    # if q is pressed, program exits\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved click detection functions loaded!\n"
     ]
    }
   ],
   "source": [
    "# Better Click Detection Methods\n",
    "\n",
    "def normalized_distance(handslms, landmark1, landmark2):\n",
    "    \"\"\"Calculate normalized distance between two landmarks\"\"\"\n",
    "    x1, y1 = handslms.landmark[landmark1].x, handslms.landmark[landmark1].y\n",
    "    x2, y2 = handslms.landmark[landmark2].x, handslms.landmark[landmark2].y\n",
    "    return ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5\n",
    "\n",
    "def is_pinching(handslms, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Better pinch detection using normalized coordinates\n",
    "    Returns True if thumb tip and index tip are close enough\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Calculate distance in normalized space\n",
    "    distance = normalized_distance(handslms, 4, 8)\n",
    "    \n",
    "    return distance < threshold\n",
    "\n",
    "def advanced_pinch_detection(handslms, threshold=0.04):\n",
    "    \"\"\"\n",
    "    More sophisticated pinch detection considering finger angles\n",
    "    \"\"\"\n",
    "    # Check if thumb and index are extended\n",
    "    thumb_tip = handslms.landmark[4]\n",
    "    thumb_mcp = handslms.landmark[2]  # Thumb MCP joint\n",
    "    index_tip = handslms.landmark[8]\n",
    "    index_pip = handslms.landmark[6]  # Index PIP joint\n",
    "    \n",
    "    # Check if fingers are extended (simple check)\n",
    "    thumb_extended = thumb_tip.y < thumb_mcp.y\n",
    "    index_extended = index_tip.y < index_pip.y\n",
    "    \n",
    "    # Only consider pinch if both fingers are somewhat extended\n",
    "    if thumb_extended and index_extended:\n",
    "        distance = normalized_distance(handslms, 4, 8)\n",
    "        return distance < threshold\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Test the improved methods\n",
    "print(\"Improved click detection functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Click detected! Distance: 0.028\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     77\u001b[39m             cv2.putText(frame, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThreshold: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclick_threshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, (\u001b[32m10\u001b[39m, \u001b[32m60\u001b[39m), \n\u001b[32m     78\u001b[39m                        cv2.FONT_HERSHEY_SIMPLEX, \u001b[32m0.7\u001b[39m, (\u001b[32m255\u001b[39m, \u001b[32m255\u001b[39m, \u001b[32m255\u001b[39m), \u001b[32m2\u001b[39m)\n\u001b[32m     80\u001b[39m     cv2.imshow(\u001b[33m'\u001b[39m\u001b[33mHand Tracking with Dynamic Line\u001b[39m\u001b[33m'\u001b[39m, frame)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m & \u001b[32m0xFF\u001b[39m == \u001b[38;5;28mord\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mq\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     82\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     84\u001b[39m cap.release()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Dynamic Line Following Thumb and Index with Normalized Distance\n",
    "cap = cv2.VideoCapture(0)\n",
    "last_click = 0\n",
    "click_threshold = 0.04  # Normalized distance threshold\n",
    "debounce_time = 0.3     # Minimum time between clicks\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    frame = cv2.flip(frame, 1)\n",
    "    h, w, c = frame.shape\n",
    "    \n",
    "    # Convert to RGB for MediaPipe\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb_frame)\n",
    "    \n",
    "    if result.multi_hand_landmarks:\n",
    "        for handslms in result.multi_hand_landmarks:\n",
    "            # Get thumb and index finger positions in FRAME coordinates (for drawing)\n",
    "            thumb_x = int(handslms.landmark[4].x * w)\n",
    "            thumb_y = int(handslms.landmark[4].y * h)\n",
    "            index_x_frame = int(handslms.landmark[8].x * w)\n",
    "            index_y_frame = int(handslms.landmark[8].y * h)\n",
    "            \n",
    "            # Move cursor to index finger tip (SCREEN coordinates)\n",
    "            index_x_screen = int(handslms.landmark[8].x * s[0])\n",
    "            index_y_screen = int(handslms.landmark[8].y * s[1])\n",
    "            pyautogui.moveTo(index_x_screen, index_y_screen, _pause=False)\n",
    "            \n",
    "            # Use YOUR normalized distance function\n",
    "            norm_distance = normalized_distance(handslms, 4, 8)\n",
    "            \n",
    "            # Dynamic line that follows thumb and index finger tips\n",
    "            # Color changes based on normalized distance\n",
    "            if norm_distance < click_threshold:\n",
    "                line_color = (0, 0, 255)  # Red when clicking distance\n",
    "                line_thickness = 4\n",
    "            elif norm_distance < 0.08:\n",
    "                line_color = (0, 165, 255)  # Orange when getting close\n",
    "                line_thickness = 3\n",
    "            else:\n",
    "                line_color = (0, 255, 0)  # Green when far apart\n",
    "                line_thickness = 2\n",
    "                \n",
    "            # Draw the dynamic line connecting thumb and index finger\n",
    "            cv2.line(frame, (thumb_x, thumb_y), (index_x_frame, index_y_frame), line_color, line_thickness)\n",
    "            \n",
    "            # Use YOUR advanced pinch detection for clicking\n",
    "            if advanced_pinch_detection(handslms, click_threshold):\n",
    "                current_time = time.time()\n",
    "                if current_time - last_click > debounce_time:\n",
    "                    pyautogui.click()\n",
    "                    last_click = current_time\n",
    "                    print(f\"Click detected! Distance: {norm_distance:.3f}\")\n",
    "            \n",
    "            # Visual feedback circles at finger tips\n",
    "            if is_pinching(handslms, click_threshold):\n",
    "                # Red circles when pinching\n",
    "                cv2.circle(frame, (thumb_x, thumb_y), 12, (0, 0, 255), -1)\n",
    "                cv2.circle(frame, (index_x_frame, index_y_frame), 12, (0, 0, 255), -1)\n",
    "                # Outer rings for emphasis\n",
    "                cv2.circle(frame, (thumb_x, thumb_y), 18, (0, 0, 255), 2)\n",
    "                cv2.circle(frame, (index_x_frame, index_y_frame), 18, (0, 0, 255), 2)\n",
    "            else:\n",
    "                # Blue circles normally\n",
    "                cv2.circle(frame, (thumb_x, thumb_y), 8, (255, 0, 0), -1)\n",
    "                cv2.circle(frame, (index_x_frame, index_y_frame), 8, (255, 0, 0), -1)\n",
    "            \n",
    "            # Draw hand landmarks\n",
    "            mpDraw.draw_landmarks(frame, handslms, mpHands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # Display normalized distance info\n",
    "            cv2.putText(frame, f\"Norm Distance: {norm_distance:.3f}\", (10, 30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "            cv2.putText(frame, f\"Threshold: {click_threshold}\", (10, 60), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "    \n",
    "    cv2.imshow('Hand Tracking with Dynamic Line', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Click detected!\n",
      "Click detected!\n",
      "Click detected!\n",
      "Click detected!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Convert to RGB for MediaPipe\u001b[39;00m\n\u001b[32m     16\u001b[39m rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m result = \u001b[43mhands\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.multi_hand_landmarks:\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m handslms \u001b[38;5;129;01min\u001b[39;00m result.multi_hand_landmarks:\n\u001b[32m     21\u001b[39m         \u001b[38;5;66;03m# Move cursor to index finger tip\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/mediapipe/python/solutions/hands.py:153\u001b[39m, in \u001b[36mHands.process\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np.ndarray) -> NamedTuple:\n\u001b[32m    133\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[32m    134\u001b[39m \n\u001b[32m    135\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    150\u001b[39m \u001b[33;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/mediapipe/python/solution_base.py:340\u001b[39m, in \u001b[36mSolutionBase.process\u001b[39m\u001b[34m(self, input_data)\u001b[39m\n\u001b[32m    334\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    335\u001b[39m     \u001b[38;5;28mself\u001b[39m._graph.add_packet_to_input_stream(\n\u001b[32m    336\u001b[39m         stream=stream_name,\n\u001b[32m    337\u001b[39m         packet=\u001b[38;5;28mself\u001b[39m._make_packet(input_stream_type,\n\u001b[32m    338\u001b[39m                                  data).at(\u001b[38;5;28mself\u001b[39m._simulated_timestamp))\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Improved Clicking with Better Pinch Detection\n",
    "cap = cv2.VideoCapture(0)\n",
    "last_click = 0\n",
    "click_threshold = 0.04  # Normalized distance threshold\n",
    "debounce_time = 0.5     # Minimum time between clicks\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    frame = cv2.flip(frame, 1)\n",
    "    x, y, c = frame.shape\n",
    "    \n",
    "    # Convert to RGB for MediaPipe\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb_frame)\n",
    "    \n",
    "    if result.multi_hand_landmarks:\n",
    "        for handslms in result.multi_hand_landmarks:\n",
    "            # Move cursor to index finger tip\n",
    "            index_x = int(handslms.landmark[8].x * s[0])\n",
    "            index_y = int(handslms.landmark[8].y * s[1])\n",
    "            pyautogui.moveTo(index_x, index_y, _pause=False)\n",
    "            \n",
    "            # Advanced pinch detection\n",
    "            if advanced_pinch_detection(handslms, click_threshold):\n",
    "                current_time = time.time()\n",
    "                if current_time - last_click > debounce_time:\n",
    "                    pyautogui.click()\n",
    "                    last_click = current_time\n",
    "                    print(\"Click detected!\")\n",
    "            \n",
    "            # Visual feedback - draw circle when pinching\n",
    "            thumb_x = int(handslms.landmark[4].x * x)\n",
    "            thumb_y = int(handslms.landmark[4].y * y)\n",
    "            index_x_frame = int(handslms.landmark[8].x * x)\n",
    "            index_y_frame = int(handslms.landmark[8].y * y)\n",
    "            \n",
    "            # Draw connection line between thumb and index\n",
    "            cv2.line(frame, (thumb_x, thumb_y), (index_x_frame, index_y_frame), (0, 255, 0), 2)\n",
    "            \n",
    "            # Change color based on pinch state\n",
    "            if is_pinching(handslms, click_threshold):\n",
    "                cv2.circle(frame, (thumb_x, thumb_y), 10, (0, 0, 255), -1)  # Red when pinching\n",
    "                cv2.circle(frame, (index_x_frame, index_y_frame), 10, (0, 0, 255), -1)\n",
    "            else:\n",
    "                cv2.circle(frame, (thumb_x, thumb_y), 8, (255, 0, 0), -1)  # Blue normally\n",
    "                cv2.circle(frame, (index_x_frame, index_y_frame), 8, (255, 0, 0), -1)\n",
    "            \n",
    "            mpDraw.draw_landmarks(frame, handslms, mpHands.HAND_CONNECTIONS)\n",
    "    \n",
    "    cv2.imshow('Output', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)# Alternative Gesture-Based Clicking Methods\n",
    "\n",
    "def finger_gun_click(handslms):\n",
    "    \"\"\"\n",
    "    Detect 'finger gun' gesture (index extended, others folded)\n",
    "    \"\"\"\n",
    "    # Check if index finger is extended and middle finger is folded\n",
    "    index_tip = handslms.landmark[8]\n",
    "    index_pip = handslms.landmark[6]\n",
    "    middle_tip = handslms.landmark[12]\n",
    "    middle_pip = handslms.landmark[10]\n",
    "    \n",
    "    index_extended = index_tip.y < index_pip.y\n",
    "    middle_folded = middle_tip.y > middle_pip.y\n",
    "    \n",
    "    return index_extended and middle_folded\n",
    "\n",
    "def peace_sign_click(handslms):\n",
    "    \"\"\"\n",
    "    Detect peace sign (index and middle extended, others folded)\n",
    "    \"\"\"\n",
    "    index_tip = handslms.landmark[8]\n",
    "    index_pip = handslms.landmark[6]\n",
    "    middle_tip = handslms.landmark[12]\n",
    "    middle_pip = handslms.landmark[10]\n",
    "    ring_tip = handslms.landmark[16]\n",
    "    ring_pip = handslms.landmark[14]\n",
    "    \n",
    "    index_extended = index_tip.y < index_pip.y\n",
    "    middle_extended = middle_tip.y < middle_pip.y\n",
    "    ring_folded = ring_tip.y > ring_pip.y\n",
    "    \n",
    "    return index_extended and middle_extended and ring_folded\n",
    "\n",
    "def thumb_up_click(handslms):\n",
    "    \"\"\"\n",
    "    Detect thumbs up gesture\n",
    "    \"\"\"\n",
    "    thumb_tip = handslms.landmark[4]\n",
    "    thumb_mcp = handslms.landmark[2]\n",
    "    index_tip = handslms.landmark[8]\n",
    "    index_pip = handslms.landmark[6]\n",
    "    \n",
    "    # Thumb extended upward, index folded\n",
    "    thumb_up = thumb_tip.y < thumb_mcp.y\n",
    "    index_folded = index_tip.y > index_pip.y\n",
    "    \n",
    "    return thumb_up and index_folded\n",
    "\n",
    "print(\"Alternative gesture detection functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_dict(dict):\n",
    "    result = 0\n",
    "    for key in dict:\n",
    "        result += dict[key]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding Finger Counter\n",
    "\n",
    "# initialize web cam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\n",
    "    # capture webcame frame and shape (width and height)\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # flip frame for mirror effect\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    h, w, c = frame.shape\n",
    "\n",
    "    # convert to rgb for mediapipe\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # have mediapipe hands predict hand landmarks\n",
    "    result = hands.process(rgb_frame)\n",
    "\n",
    "    # dict of fingers\n",
    "    fingers = {\"4\": 0, \"8\": 0, \"12\": 0, \"16\": 0, \"20\": 0}\n",
    "\n",
    "\n",
    "    # iterate through the predicted landmarks adjusting them to the window, and\n",
    "    # and outputting them to the opencv window\n",
    "    if result.multi_hand_landmarks:\n",
    "        for handslms in result.multi_hand_landmarks:\n",
    "\n",
    "            # thumb coordinates\n",
    "\n",
    "            # a note here is we have to scale coordinates relative to the frame (height, width)\n",
    "            x4, y4 = int(handslms.landmark[4].x * w), int(handslms.landmark[4].y * h)\n",
    "            x2, y2 = int(handslms.landmark[2].x * w), int(handslms.landmark[2].y * h)\n",
    "\n",
    "            # check if thumb is up\n",
    "            if x4 > x2:\n",
    "                fingers['4'] = 1\n",
    "            \n",
    "            # index finger coordinates\n",
    "            x8, y8 = int(handslms.landmark[8].x * w), int(handslms.landmark[8].y * h)\n",
    "            x6, y6 = int(handslms.landmark[6].x * w), int(handslms.landmark[6].y * h)\n",
    "\n",
    "            if y8 < y6:\n",
    "                fingers['8'] = 1\n",
    "            \n",
    "            # middle finger\n",
    "            x12, y12 = int(handslms.landmark[12].x * w), int(handslms.landmark[12].y * h)\n",
    "            x10, y10 = int(handslms.landmark[10].x * w), int(handslms.landmark[10].y * h)\n",
    "\n",
    "            if y12 < y10:\n",
    "                fingers['12'] = 1\n",
    "            \n",
    "            # ring finger\n",
    "            x16, y16 = int(handslms.landmark[16].x * w), int(handslms.landmark[16].y * h)\n",
    "            x14, y14 = int(handslms.landmark[14].x * w), int(handslms.landmark[14].y * h)\n",
    "\n",
    "            if y16 < y14:\n",
    "                fingers['16'] = 1\n",
    "            \n",
    "            # pinky finger\n",
    "            x20, y20 = int(handslms.landmark[20].x * w), int(handslms.landmark[20].y * h)\n",
    "            x18, y18 = int(handslms.landmark[18].x * w), int(handslms.landmark[18].y * h)\n",
    "        \n",
    "            if y20 < y18:\n",
    "                fingers['20'] = 1\n",
    "\n",
    "            mpDraw.draw_landmarks(frame, handslms, mpHands.HAND_CONNECTIONS)\n",
    "\n",
    "    # count raised fingers\n",
    "    number_fingers = sum_dict(fingers)\n",
    "\n",
    "    cv2.rectangle(frame, (25, 150), (100, 400), (0, 128, 0), cv2.FILLED)\n",
    "    cv2.putText(frame, str(number_fingers), (35, 300), cv2.FONT_HERSHEY_PLAIN,\n",
    "                3, (0, 71, 71), 2)\n",
    "    \n",
    "    cv2.imshow('Output', frame)\n",
    "    # if q is pressed, program exits\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Article if you have trouble understanding! https://medium.com/@Mert.A/how-to-create-a-finger-counter-with-python-and-mediapipe-cc6c3911ad09"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
