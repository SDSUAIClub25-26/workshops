{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDJW1BSOB9ZE"
      },
      "source": [
        "### Hand Tracking with OpenCV and Mediapipe!\n",
        "\n",
        "**Steps to Get Started:**\n",
        "1. Ensure you have Python 3.11.9 installed on your computer (we will go through how to set it up)\n",
        "2. Create a virtual environment using this python version\n",
        "3. pip install requirements.txt (ensures no issues with different versions)\n",
        "4. Once you've installed the necessary packages, you're ready to begin!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NH6Kyw1IByMR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Everything imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import pyautogui\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "print(\"Everything imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sVn2EyAlByCc"
      },
      "outputs": [],
      "source": [
        "mpHands = mp.solutions.hands\n",
        "hands = mpHands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
        "mpDraw = mp.solutions.drawing_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EbXGynokB3yy"
      },
      "outputs": [],
      "source": [
        "s = pyautogui.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RoloM2K5BLuy"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize webcam\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    # Capture a frame from the webcam\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Flip the frame horizontally for a mirror effect\n",
        "    frame = cv2.flip(frame, 1)\n",
        "\n",
        "    # Display the frame in a window titled 'Output'\n",
        "    cv2.imshow('Output', frame)\n",
        "\n",
        "    # Exit the loop if 'q' is pressed in the OpenCV window\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release the webcam and close any OpenCV windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "cv2.waitKey(1) # delay of 1 second each time it reads a key press"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ksm0t04TBcSG"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Hand tracking\n",
        "# Initialize webcam\n",
        "cap = cv2.VideoCapture(0)\n",
        "while True:\n",
        "    # capture webcame frame and shape (width and height)\n",
        "    ret, frame = cap.read()\n",
        "    # defining the frame\n",
        "    frame = cv2.flip(frame, 1)\n",
        "    # -------- ADD THIS CODE NEXT ---------------\n",
        "    # have mediapipe hands predict hand landmarks\n",
        "    x, y, c = frame.shape\n",
        "    result = hands.process(frame)\n",
        "    # iterate through the predicted landmarks adjusting them to the window, and\n",
        "    # and outputting them to the opencv window\n",
        "    if result.multi_hand_landmarks:\n",
        "        landmarks = []\n",
        "        for handslms in result.multi_hand_landmarks:\n",
        "            for lm in handslms.landmark:\n",
        "                lmx = int(lm.x * x)\n",
        "                lmy = int(lm.y * y)\n",
        "                landmarks.append([lmx, lmy])\n",
        "            mpDraw.draw_landmarks(frame, handslms, mpHands.HAND_CONNECTIONS)\n",
        "    # ------- FINISHED ADDING NEW CODE ----------\n",
        "    cv2.imshow('Output', frame)\n",
        "    # if q is pressed, program exits\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "cv2.waitKey(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OORJWznsB8mt"
      },
      "source": [
        "<img src=\"hand_tracking_landmarks.webp\" alt=\"Hand Tracker Visualization with Mediapipe\" width=\"400\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "k4Hu-vEIBkw1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Moving Cursor\n",
        "# Initialize webcam\n",
        "cap = cv2.VideoCapture(0)\n",
        "while True:\n",
        "    # capture webcame frame and shape (width and height)\n",
        "    ret, frame = cap.read()\n",
        "    # defining the frame\n",
        "    frame = cv2.flip(frame, 1)\n",
        "    x, y, c = frame.shape\n",
        "    # have mediapipe hands predict hand landmarks\n",
        "    result = hands.process(frame)\n",
        "    # iterate through the predicted landmarks adjusting them to the window, and\n",
        "    # and outputting them to the opencv window\n",
        "    if result.multi_hand_landmarks:\n",
        "        landmarks = []\n",
        "        for handslms in result.multi_hand_landmarks:\n",
        "            # take the 8th landmark (index finger point) and move the cursor to that landmarks x and y value\n",
        "            # ADD THE LINE BELOW NEXT\n",
        "            index_x = int(handslms.landmark[8].x * s[0])\n",
        "            index_y = int(handslms.landmark[8].y * s[1])\n",
        "            pyautogui.moveTo(index_x, index_y, _pause=False)\n",
        "            for lm in handslms.landmark:\n",
        "                lmx = int(lm.x * x)\n",
        "                lmy = int(lm.y * y)\n",
        "                landmarks.append([lmx, lmy])\n",
        "            mpDraw.draw_landmarks(frame, handslms, mpHands.HAND_CONNECTIONS)\n",
        "\n",
        "    cv2.imshow('Output', frame)\n",
        "    # if q is pressed, program exits\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "cv2.waitKey(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xMMQnEgtBliQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Clicking\n",
        "# Initialize webcam\n",
        "cap = cv2.VideoCapture(0)\n",
        "# -------- ADD THIS VARIABLE ---------------\n",
        "last_click = 0\n",
        "while True:\n",
        "    # capture webcame frame and shape (width and height)\n",
        "    ret, frame = cap.read()\n",
        "    x, y, c = frame.shape\n",
        "    # defining the frame\n",
        "    frame = cv2.flip(frame, 1)\n",
        "    # have mediapipe hands predict hand landmarks\n",
        "    result = hands.process(frame)\n",
        "    # iterate through the predicted landmarks adjusting them to the window, and\n",
        "    # and outputting them to the opencv window\n",
        "    if result.multi_hand_landmarks:\n",
        "        landmarks = []\n",
        "        for handslms in result.multi_hand_landmarks:\n",
        "            # take the 8th landmark (index finger point) and move the cursor to that landmarks x and y value\n",
        "            pyautogui.moveTo(int(handslms.landmark[8].x * s[0]), int(handslms.landmark[8].y * s[1]), _pause=False)\n",
        "\n",
        "            # -------- ADD THIS CODE NEXT ---------------\n",
        "            # Detect click gesture by calculating distance between thumb (landmark 4) and index (landmark 8)\n",
        "            thumb_x = int(handslms.landmark[4].x * s[0])\n",
        "            thumb_y = int(handslms.landmark[4].y * s[1])\n",
        "            distance = ((index_x - thumb_x) ** 2 + (index_y - thumb_y) ** 2) ** 0.5\n",
        "\n",
        "            # If distance is small enough, simulate a click\n",
        "            if distance < 40 and time.time() - last_click > 0.5:\n",
        "                pyautogui.click()\n",
        "                last_click = time.time()\n",
        "            # ------- FINISHED ADDING NEW CODE ----------\n",
        "\n",
        "            # for lm in handslms.landmark:\n",
        "            #     lmx = int(lm.x * x)\n",
        "            #     lmy = int(lm.y * y)\n",
        "            #     landmarks.append([lmx, lmy])\n",
        "            mpDraw.draw_landmarks(frame, handslms, mpHands.HAND_CONNECTIONS)\n",
        "    cv2.imshow('Output', frame)\n",
        "    # if q is pressed, program exits\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "cv2.waitKey(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sum_dict(dict):\n",
        "    result = 0\n",
        "    for key in dict:\n",
        "        result += dict[key]\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Adding Finger Counter\n",
        "\n",
        "# initialize web cam\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "\n",
        "    # capture webcame frame and shape (width and height)\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # flip frame for mirror effect\n",
        "    frame = cv2.flip(frame, 1)\n",
        "\n",
        "    h, w, c = frame.shape\n",
        "\n",
        "    # convert to rgb for mediapipe\n",
        "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # have mediapipe hands predict hand landmarks\n",
        "    result = hands.process(rgb_frame)\n",
        "\n",
        "    # dict of fingers\n",
        "    fingers = {\"4\": 0, \"8\": 0, \"12\": 0, \"16\": 0, \"20\": 0}\n",
        "\n",
        "\n",
        "    # iterate through the predicted landmarks adjusting them to the window, and\n",
        "    # and outputting them to the opencv window\n",
        "    if result.multi_hand_landmarks:\n",
        "        for handslms in result.multi_hand_landmarks:\n",
        "\n",
        "            # thumb coordinates\n",
        "\n",
        "            # a note here is we have to scale coordinates relative to the frame (height, width)\n",
        "            x4, y4 = int(handslms.landmark[4].x * w), int(handslms.landmark[4].y * h)\n",
        "            x2, y2 = int(handslms.landmark[2].x * w), int(handslms.landmark[2].y * h)\n",
        "\n",
        "            # check if thumb is up\n",
        "            if x4 > x2:\n",
        "                fingers['4'] = 1\n",
        "            \n",
        "            # index finger coordinates\n",
        "            x8, y8 = int(handslms.landmark[8].x * w), int(handslms.landmark[8].y * h)\n",
        "            x6, y6 = int(handslms.landmark[6].x * w), int(handslms.landmark[6].y * h)\n",
        "\n",
        "            if y8 < y6:\n",
        "                fingers['8'] = 1\n",
        "            \n",
        "            # middle finger\n",
        "            x12, y12 = int(handslms.landmark[12].x * w), int(handslms.landmark[12].y * h)\n",
        "            x10, y10 = int(handslms.landmark[10].x * w), int(handslms.landmark[10].y * h)\n",
        "\n",
        "            if y12 < y10:\n",
        "                fingers['12'] = 1\n",
        "            \n",
        "            # ring finger\n",
        "            x16, y16 = int(handslms.landmark[16].x * w), int(handslms.landmark[16].y * h)\n",
        "            x14, y14 = int(handslms.landmark[14].x * w), int(handslms.landmark[14].y * h)\n",
        "\n",
        "            if y16 < y14:\n",
        "                fingers['16'] = 1\n",
        "            \n",
        "            # pinky finger\n",
        "            x20, y20 = int(handslms.landmark[20].x * w), int(handslms.landmark[20].y * h)\n",
        "            x18, y18 = int(handslms.landmark[18].x * w), int(handslms.landmark[18].y * h)\n",
        "        \n",
        "            if y20 < y18:\n",
        "                fingers['20'] = 1\n",
        "\n",
        "            mpDraw.draw_landmarks(frame, handslms, mpHands.HAND_CONNECTIONS)\n",
        "\n",
        "    # count raised fingers\n",
        "    number_fingers = sum_dict(fingers)\n",
        "\n",
        "    cv2.rectangle(frame, (25, 150), (100, 400), (0, 128, 0), cv2.FILLED)\n",
        "    cv2.putText(frame, str(number_fingers), (35, 300), cv2.FONT_HERSHEY_PLAIN,\n",
        "                3, (0, 71, 71), 2)\n",
        "    \n",
        "    cv2.imshow('Output', frame)\n",
        "    # if q is pressed, program exits\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "cv2.waitKey(1)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Article if you have trouble understanding! https://medium.com/@Mert.A/how-to-create-a-finger-counter-with-python-and-mediapipe-cc6c3911ad09"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
