{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDJW1BSOB9ZE"
   },
   "source": [
    "### Hand Tracking with OpenCV and Mediapipe!\n",
    "\n",
    "**Steps to Get Started:** (Also reference README within the github repo!)\n",
    "1. Clone this repository\n",
    "2. Ensure you have Python 3.11.9 installed on your computer (we will go through how to set it up)\n",
    "3. Create a virtual environment using this python version\n",
    "4. pip install requirements.txt (ensures no issues with different versions)\n",
    "5. Run the first code block with imports to ensure it works properly\n",
    "5. Once you've installed the necessary packages, you're ready to begin!\n",
    "\n",
    "Helpful Article:\n",
    "https://medium.com/@Mert.A/how-to-create-a-finger-counter-with-python-and-mediapipe-cc6c3911ad09\n",
    "\n",
    "Helpful Video:\n",
    "https://www.youtube.com/watch?v=RRBXVu5UE-U&t=78s\n",
    "\n",
    "Docs:\n",
    "https://mediapipe.readthedocs.io/en/latest/solutions/hands.html#:~:text=%7C-,ML%20Pipeline,a%20dedicated%20hand%20renderer%20subgraph.\n",
    "\n",
    "Setting Up Python in VS Code:\n",
    "https://code.visualstudio.com/docs/python/python-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NH6Kyw1IByMR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pyautogui\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(\"Everything imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Hand Detection Model with Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sVn2EyAlByCc"
   },
   "outputs": [],
   "source": [
    "mpHands = mp.solutions.hands\n",
    "hands = mpHands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
    "mpDraw = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EbXGynokB3yy"
   },
   "outputs": [],
   "source": [
    "s = pyautogui.size() # store size of gui screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RoloM2K5BLuy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture a frame from the webcam\n",
    "    # ret is a boolean of whether frame was read, frame is numpy array in BGR format\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip the frame horizontally for a mirror effect\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Display the frame in a window titled 'Output'\n",
    "    cv2.imshow('Output', frame)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed in the OpenCV window\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close any OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1) # delay of 1 second each time it reads a key press"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand Tracking with Webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ksm0t04TBcSG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hand tracking\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # capture webcame frame and shape (width and height)\n",
    "    ret, frame = cap.read()\n",
    "    # defining the frame\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    # -------- ADD THIS CODE NEXT ---------------\n",
    "    # have mediapipe hands predict hand landmarks\n",
    "    x, y, c = frame.shape\n",
    "    result = hands.process(frame)\n",
    "\n",
    "    # check if predicted landmarks were detected\n",
    "    # if so draw & output them to the opencv window\n",
    "    if result.multi_hand_landmarks:\n",
    "        handLandmarks = result.multi_hand_landmarks[0]\n",
    "\n",
    "        # draw landmarks on the screen\n",
    "        mpDraw.draw_landmarks(frame, handLandmarks, mpHands.HAND_CONNECTIONS)\n",
    "    # ------- FINISHED ADDING NEW CODE ----------\n",
    "    cv2.imshow('Output', frame)\n",
    "    # if q is pressed, program exits\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OORJWznsB8mt"
   },
   "source": [
    "<img src=\"hand_tracking_landmarks.webp\" alt=\"Hand Tracker Visualization with Mediapipe\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Cursor Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "k4Hu-vEIBkw1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Moving Cursor\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # capture webcame frame and shape (width and height)\n",
    "    ret, frame = cap.read()\n",
    "    # defining the frame\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    x, y, c = frame.shape\n",
    "    # have mediapipe hands predict hand landmarks\n",
    "    result = hands.process(frame)\n",
    "\n",
    "    # check if predicted landmarks were detected\n",
    "    # if so draw & output them to the opencv window\n",
    "    if result.multi_hand_landmarks:\n",
    "\n",
    "        # grab landmarks of first instance of hand detected\n",
    "        handLandmarks = result.multi_hand_landmarks[0]\n",
    "\n",
    "        # take the 8th landmark (index finger point) and move the cursor to that landmarks x and y value\n",
    "        index_x = int(handLandmarks.landmark[8].x * s[0]) # grab the x-coordinate of the index finger, scaled to screen width\n",
    "        index_y = int(handLandmarks.landmark[8].y * s[1]) # grab the y-coordinate of the index finger, scaled to the screen height\n",
    "        pyautogui.moveTo(index_x, index_y, _pause=False)\n",
    "        \n",
    "        # draw landmarks onto opencv window as before\n",
    "        mpDraw.draw_landmarks(frame, handLandmarks, mpHands.HAND_CONNECTIONS)\n",
    "\n",
    "    cv2.imshow('Output', frame)\n",
    "    # if q is pressed, program exits\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clicking Using Hand Gestures (Pinch Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved click detection functions loaded!\n"
     ]
    }
   ],
   "source": [
    "# some helper functions to enable better pinch detection since was buggy before\n",
    "\n",
    "def normalized_distance(handslms, landmark1, landmark2):\n",
    "    \"\"\"Calculate normalized distance between two landmarks\"\"\"\n",
    "    x1, y1 = handslms.landmark[landmark1].x, handslms.landmark[landmark1].y\n",
    "    x2, y2 = handslms.landmark[landmark2].x, handslms.landmark[landmark2].y\n",
    "    return ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5 # return distance between two\n",
    "\n",
    "def is_pinching(handslms, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Better pinch detection using normalized coordinates\n",
    "    Returns True if thumb tip and index tip are close enough\n",
    "    \"\"\"\n",
    "    # Calculate distance in normalized space\n",
    "    distance = normalized_distance(handslms, 4, 8)\n",
    "    \n",
    "    return distance < threshold\n",
    "\n",
    "def advanced_pinch_detection(handslms, threshold=0.04):\n",
    "    \"\"\"\n",
    "    More sophisticated pinch detection considering finger angles\n",
    "    \"\"\"\n",
    "    # Check if thumb and index are extended\n",
    "    thumb_tip = handslms.landmark[4]\n",
    "    thumb_mcp = handslms.landmark[2]  # Thumb MCP joint\n",
    "    index_tip = handslms.landmark[8]\n",
    "    index_pip = handslms.landmark[6]  # Index PIP joint\n",
    "    \n",
    "    # Check if fingers are extended (simple check)\n",
    "    thumb_extended = thumb_tip.y < thumb_mcp.y\n",
    "    index_extended = index_tip.y < index_pip.y\n",
    "    \n",
    "    # Only consider pinch if both fingers are somewhat extended\n",
    "    if thumb_extended and index_extended:\n",
    "        distance = normalized_distance(handslms, 4, 8)\n",
    "        return distance < threshold\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Test the improved methods\n",
    "print(\"Improved click detection functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "xMMQnEgtBliQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Clicking\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "# -------- ADD THIS VARIABLE ---------------\n",
    "last_click = 0 # keeps track of time of last click\n",
    "while True:\n",
    "    # capture webcame frame and shape (width and height)\n",
    "    ret, frame = cap.read()\n",
    "    x, y, c = frame.shape\n",
    "    # defining the frame\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    # have mediapipe hands predict hand landmarks\n",
    "    result = hands.process(frame)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        # grab landmarks of first instance of hand detected\n",
    "        handLandmarks = result.multi_hand_landmarks[0]\n",
    "\n",
    "        # take the 8th landmark (index finger point) and move the cursor to that landmarks x and y value as done above\n",
    "        pyautogui.moveTo(index_x, index_y, _pause=False)\n",
    "\n",
    "        # Draw line between thumb tip (4) and index tip (8)\n",
    "        # in this case we are scaling to the camera frame since we are drawing\n",
    "        \n",
    "        thumb_x = int(handLandmarks.landmark[4].x * s[0])\n",
    "        thumb_y = int(handLandmarks.landmark[4].y * s[1])\n",
    "        index_x = int(handLandmarks.landmark[8].x * s[0])\n",
    "        index_y = int(handLandmarks.landmark[8].y * s[1])\n",
    "        cv2.line(frame, (thumb_x, thumb_y), (index_x, index_y), (0, 255, 0), 3) # color is green and width is 3\n",
    "\n",
    "        # Detect click gesture by calculating distance between thumb (landmark 4) and index (landmark 8)\n",
    "        # If distance is within threshold & time between clicks is greater than .5 milliseconds, simulate a click\n",
    "\n",
    "        if is_pinching(handLandmarks) and time.time() - last_click > 0.5:\n",
    "            pyautogui.click()\n",
    "            last_click = time.time()\n",
    "\n",
    "        # **NOTE**: You can replace this function with the more advanced one and try it out!\n",
    "        \n",
    "        # ------- FINISHED ADDING NEW CODE ----------\n",
    "\n",
    "        mpDraw.draw_landmarks(frame, handLandmarks, mpHands.HAND_CONNECTIONS)\n",
    "    cv2.imshow('Output', frame)\n",
    "    # if q is pressed, program exits\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finger Counter Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_dict(dict):\n",
    "    result = 0\n",
    "    for key in dict:\n",
    "        result += dict[key]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding Finger Counter\n",
    "\n",
    "# initialize web cam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\n",
    "    # capture webcame frame and shape (width and height)\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # flip frame for mirror effect\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    h, w, c = frame.shape\n",
    "\n",
    "    # convert from bgr to rgb for mediapipe, this is optional\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # have mediapipe hands predict hand landmarks\n",
    "    result = hands.process(rgb_frame)\n",
    "\n",
    "    # dict of fingers where key is landmark and value is 1 or 0 (detected or not)\n",
    "    fingers = {\"4\": 0, \"8\": 0, \"12\": 0, \"16\": 0, \"20\": 0}\n",
    "\n",
    "\n",
    "    # check if predicted landmarks were detected\n",
    "    # if so draw & output them to the opencv window\n",
    "    if result.multi_hand_landmarks:\n",
    "        \n",
    "        handLandmarks = result.multi_hand_landmarks[0]\n",
    "        # thumb coordinates\n",
    "\n",
    "        # a note here is we have to scale coordinates relative to the frame (height, width)\n",
    "        x4, y4 = int(handLandmarks.landmark[4].x * w), int(handLandmarks.landmark[4].y * h)\n",
    "        x2, y2 = int(handLandmarks.landmark[2].x * w), int(handLandmarks.landmark[2].y * h)\n",
    "\n",
    "        # check if thumb is up\n",
    "        if x4 > x2:\n",
    "            fingers['4'] = 1\n",
    "        \n",
    "        # index finger coordinates\n",
    "        x8, y8 = int(handLandmarks.landmark[8].x * w), int(handLandmarks.landmark[8].y * h)\n",
    "        x6, y6 = int(handLandmarks.landmark[6].x * w), int(handLandmarks.landmark[6].y * h)\n",
    "\n",
    "        if y8 < y6:\n",
    "            fingers['8'] = 1\n",
    "        \n",
    "        # middle finger\n",
    "        x12, y12 = int(handLandmarks.landmark[12].x * w), int(handLandmarks.landmark[12].y * h)\n",
    "        x10, y10 = int(handLandmarks.landmark[10].x * w), int(handLandmarks.landmark[10].y * h)\n",
    "\n",
    "        if y12 < y10:\n",
    "            fingers['12'] = 1\n",
    "        \n",
    "        # ring finger\n",
    "        x16, y16 = int(handLandmarks.landmark[16].x * w), int(handLandmarks.landmark[16].y * h)\n",
    "        x14, y14 = int(handLandmarks.landmark[14].x * w), int(handLandmarks.landmark[14].y * h)\n",
    "\n",
    "        if y16 < y14:\n",
    "            fingers['16'] = 1\n",
    "        \n",
    "        # pinky finger\n",
    "        x20, y20 = int(handLandmarks.landmark[20].x * w), int(handLandmarks.landmark[20].y * h)\n",
    "        x18, y18 = int(handLandmarks.landmark[18].x * w), int(handLandmarks.landmark[18].y * h)\n",
    "    \n",
    "        if y20 < y18:\n",
    "            fingers['20'] = 1\n",
    "\n",
    "        mpDraw.draw_landmarks(frame, handLandmarks, mpHands.HAND_CONNECTIONS)\n",
    "\n",
    "    # count raised fingers\n",
    "    number_fingers = sum_dict(fingers)\n",
    "\n",
    "    # draw finger counter visual\n",
    "    cv2.rectangle(frame, (25, 150), (100, 400), (0, 128, 0), cv2.FILLED)\n",
    "    cv2.putText(frame, str(number_fingers), (35, 300), cv2.FONT_HERSHEY_PLAIN,\n",
    "                3, (0, 71, 71), 2)\n",
    "    \n",
    "    cv2.imshow('Output', frame)\n",
    "    # if q is pressed, program exits\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Article if you have trouble understanding! https://medium.com/@Mert.A/how-to-create-a-finger-counter-with-python-and-mediapipe-cc6c3911ad09\n",
    "\n",
    "Hope you enjoyed this workshop!!\n",
    "\n",
    "\n",
    "Some food for thought:\n",
    "- Can you modify this workshop to work for two hands? If so, what different functionality can you add?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
