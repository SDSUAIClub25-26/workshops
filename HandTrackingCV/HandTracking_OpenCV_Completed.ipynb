{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDJW1BSOB9ZE"
   },
   "source": [
    "### Hand Tracking with OpenCV and Mediapipe!\n",
    "\n",
    "**Steps to Get Started:** (Also reference README within this folder)\n",
    "1. Clone this repository\n",
    "2. Ensure you have Python 3.11.9 installed on your computer (we will go through how to set it up)\n",
    "3. Create a virtual environment using this python version\n",
    "4. pip install requirements.txt (ensures no issues with different versions)\n",
    "5. Run the first code block with imports to ensure it works properly\n",
    "5. Once you've installed the necessary packages, you're ready to begin!\n",
    "\n",
    "Helpful Article:\n",
    "https://medium.com/@Mert.A/how-to-create-a-finger-counter-with-python-and-mediapipe-cc6c3911ad09\n",
    "\n",
    "Helpful Video:\n",
    "https://www.youtube.com/watch?v=RRBXVu5UE-U&t=78s\n",
    "\n",
    "Docs:\n",
    "https://mediapipe.readthedocs.io/en/latest/solutions/hands.html#:~:text=%7C-,ML%20Pipeline,a%20dedicated%20hand%20renderer%20subgraph.\n",
    "\n",
    "Setting Up Python in VS Code:\n",
    "https://code.visualstudio.com/docs/python/python-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "NH6Kyw1IByMR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pyautogui\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(\"Everything imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "sVn2EyAlByCc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1758054556.469449  742732 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2 Pro\n"
     ]
    }
   ],
   "source": [
    "mpHands = mp.solutions.hands\n",
    "hands = mpHands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
    "mpDraw = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "EbXGynokB3yy"
   },
   "outputs": [],
   "source": [
    "s = pyautogui.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "RoloM2K5BLuy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1758054556.518222  766919 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1758054556.541326  766928 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture a frame from the webcam\n",
    "    # ret is a boolean of whether frame was read, frame is numpy array in BGR format\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip the frame horizontally for a mirror effect\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Display the frame in a window titled 'Output'\n",
    "    cv2.imshow('Output', frame)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed in the OpenCV window\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close any OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1) # delay of 1 second each time it reads a key press"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ksm0t04TBcSG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hand tracking\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # capture webcame frame and shape (width and height)\n",
    "    ret, frame = cap.read()\n",
    "    # defining the frame\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    # -------- ADD THIS CODE NEXT ---------------\n",
    "    # have mediapipe hands predict hand landmarks\n",
    "    x, y, c = frame.shape\n",
    "    result = hands.process(frame)\n",
    "    # iterate through the predicted landmarks adjusting them to the window, and\n",
    "    # and outputting them to the opencv window\n",
    "    if result.multi_hand_landmarks:\n",
    "        landmarks = []\n",
    "        for handslms in result.multi_hand_landmarks:\n",
    "            for lm in handslms.landmark:\n",
    "                lmx = int(lm.x * x)\n",
    "                lmy = int(lm.y * y)\n",
    "                landmarks.append([lmx, lmy])\n",
    "            mpDraw.draw_landmarks(frame, handslms, mpHands.HAND_CONNECTIONS)\n",
    "    # ------- FINISHED ADDING NEW CODE ----------\n",
    "    cv2.imshow('Output', frame)\n",
    "    # if q is pressed, program exits\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OORJWznsB8mt"
   },
   "source": [
    "<img src=\"hand_tracking_landmarks.webp\" alt=\"Hand Tracker Visualization with Mediapipe\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "k4Hu-vEIBkw1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Moving Cursor\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # capture webcame frame and shape (width and height)\n",
    "    ret, frame = cap.read()\n",
    "    # defining the frame\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    x, y, c = frame.shape\n",
    "    # have mediapipe hands predict hand landmarks\n",
    "    result = hands.process(frame)\n",
    "    # iterate through the predicted landmarks adjusting them to the window, and\n",
    "    # and outputting them to the opencv window\n",
    "    if result.multi_hand_landmarks:\n",
    "        landmarks = []\n",
    "        for handslms in result.multi_hand_landmarks:\n",
    "            # take the 8th landmark (index finger point) and move the cursor to that landmarks x and y value\n",
    "            # ADD THE LINE BELOW NEXT\n",
    "            index_x = int(handslms.landmark[8].x * s[0])\n",
    "            index_y = int(handslms.landmark[8].y * s[1])\n",
    "            pyautogui.moveTo(index_x, index_y, _pause=False)\n",
    "            for lm in handslms.landmark:\n",
    "                lmx = int(lm.x * x)\n",
    "                lmy = int(lm.y * y)\n",
    "                landmarks.append([lmx, lmy])\n",
    "            mpDraw.draw_landmarks(frame, handslms, mpHands.HAND_CONNECTIONS)\n",
    "\n",
    "    cv2.imshow('Output', frame)\n",
    "    # if q is pressed, program exits\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "xMMQnEgtBliQ"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m last_click = \u001b[32m0\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# capture webcame frame and shape (width and height)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     ret, frame = \u001b[43mcap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     x, y, c = frame.shape\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# defining the frame\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#Clicking\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "# -------- ADD THIS VARIABLE ---------------\n",
    "last_click = 0\n",
    "while True:\n",
    "    # capture webcame frame and shape (width and height)\n",
    "    ret, frame = cap.read()\n",
    "    x, y, c = frame.shape\n",
    "    # defining the frame\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    # have mediapipe hands predict hand landmarks\n",
    "    result = hands.process(frame)\n",
    "    # iterate through the predicted landmarks adjusting them to the window, and\n",
    "    # and outputting them to the opencv window\n",
    "    if result.multi_hand_landmarks:\n",
    "        landmarks = []\n",
    "        for handslms in result.multi_hand_landmarks:\n",
    "            # take the 8th landmark (index finger point) and move the cursor to that landmarks x and y value\n",
    "            pyautogui.moveTo(int(handslms.landmark[8].x * s[0]), int(handslms.landmark[8].y * s[1]), _pause=False)\n",
    "\n",
    "            # -------- ADD THIS CODE NEXT ---------------\n",
    "            # Detect click gesture by calculating distance between thumb (landmark 4) and index (landmark 8)\n",
    "            thumb_x = int(handslms.landmark[4].x * s[0])\n",
    "            thumb_y = int(handslms.landmark[4].y * s[1])\n",
    "            distance = ((index_x - thumb_x) ** 2 + (index_y - thumb_y) ** 2) ** 0.5\n",
    "\n",
    "            # If distance is small enough, simulate a click\n",
    "            if distance < 40 and time.time() - last_click > 0.5:\n",
    "                pyautogui.click()\n",
    "                last_click = time.time()\n",
    "            # ------- FINISHED ADDING NEW CODE ----------\n",
    "\n",
    "            # for lm in handslms.landmark:\n",
    "            #     lmx = int(lm.x * x)\n",
    "            #     lmy = int(lm.y * y)\n",
    "            #     landmarks.append([lmx, lmy])\n",
    "            mpDraw.draw_landmarks(frame, handslms, mpHands.HAND_CONNECTIONS)\n",
    "    cv2.imshow('Output', frame)\n",
    "    # if q is pressed, program exits\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved click detection functions loaded!\n"
     ]
    }
   ],
   "source": [
    "# Better Click Detection Methods\n",
    "\n",
    "def normalized_distance(handslms, landmark1, landmark2):\n",
    "    \"\"\"Calculate normalized distance between two landmarks\"\"\"\n",
    "    x1, y1 = handslms.landmark[landmark1].x, handslms.landmark[landmark1].y\n",
    "    x2, y2 = handslms.landmark[landmark2].x, handslms.landmark[landmark2].y\n",
    "    return ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5\n",
    "\n",
    "def is_pinching(handslms, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Better pinch detection using normalized coordinates\n",
    "    Returns True if thumb tip and index tip are close enough\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Calculate distance in normalized space\n",
    "    distance = normalized_distance(handslms, 4, 8)\n",
    "    \n",
    "    return distance < threshold\n",
    "\n",
    "def advanced_pinch_detection(handslms, threshold=0.04):\n",
    "    \"\"\"\n",
    "    More sophisticated pinch detection considering finger angles\n",
    "    \"\"\"\n",
    "    # Check if thumb and index are extended\n",
    "    thumb_tip = handslms.landmark[4]\n",
    "    thumb_mcp = handslms.landmark[2]  # Thumb MCP joint\n",
    "    index_tip = handslms.landmark[8``]\n",
    "    index_pip = handslms.landmark[6]  # Index PIP joint\n",
    "    \n",
    "    # Check if fingers are extended (simple check)\n",
    "    thumb_extended = thumb_tip.y < thumb_mcp.y\n",
    "    index_extended = index_tip.y < index_pip.y\n",
    "    \n",
    "    # Only consider pinch if both fingers are somewhat extended\n",
    "    if thumb_extended and index_extended:\n",
    "        distance = normalized_distance(handslms, 4, 8)\n",
    "        return distance < threshold\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Test the improved methods\n",
    "print(\"Improved click detection functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Click detected! Distance: 0.039\n",
      "Click detected! Distance: 0.037\n",
      "Click detected! Distance: 0.037\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m debounce_time = \u001b[32m0.3\u001b[39m     \u001b[38;5;66;03m# Minimum time between clicks\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     ret, frame = \u001b[43mcap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[32m     10\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Dynamic Line Following Thumb and Index with Normalized Distance\n",
    "cap = cv2.VideoCapture(0)\n",
    "last_click = 0\n",
    "click_threshold = 0.04  # Normalized distance threshold\n",
    "debounce_time = 0.3     # Minimum time between clicks\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    frame = cv2.flip(frame, 1)\n",
    "    h, w, c = frame.shape\n",
    "    \n",
    "    # Convert to RGB for MediaPipe\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb_frame)\n",
    "    \n",
    "    if result.multi_hand_landmarks:\n",
    "        for handslms in result.multi_hand_landmarks:\n",
    "            # Get thumb and index finger positions in FRAME coordinates (for drawing)\n",
    "            thumb_x = int(handslms.landmark[4].x * w)\n",
    "            thumb_y = int(handslms.landmark[4].y * h)\n",
    "            index_x_frame = int(handslms.landmark[8].x * w)\n",
    "            index_y_frame = int(handslms.landmark[8].y * h)\n",
    "            \n",
    "            # Move cursor to index finger tip (SCREEN coordinates)\n",
    "            index_x_screen = int(handslms.landmark[8].x * s[0])\n",
    "            index_y_screen = int(handslms.landmark[8].y * s[1])\n",
    "            pyautogui.moveTo(index_x_screen, index_y_screen, _pause=False)\n",
    "            \n",
    "            # Use YOUR normalized distance function\n",
    "            norm_distance = normalized_distance(handslms, 4, 8)\n",
    "            \n",
    "            # Dynamic line that follows thumb and index finger tips\n",
    "            # Color changes based on normalized distance\n",
    "            if norm_distance < click_threshold:\n",
    "                line_color = (0, 0, 255)  # Red when clicking distance\n",
    "                line_thickness = 4\n",
    "            elif norm_distance < 0.08:\n",
    "                line_color = (0, 165, 255)  # Orange when getting close\n",
    "                line_thickness = 3\n",
    "            else:\n",
    "                line_color = (0, 255, 0)  # Green when far apart\n",
    "                line_thickness = 2\n",
    "                \n",
    "            # Draw the dynamic line connecting thumb and index finger\n",
    "            cv2.line(frame, (thumb_x, thumb_y), (index_x_frame, index_y_frame), line_color, line_thickness)\n",
    "            \n",
    "            # Use YOUR advanced pinch detection for clicking\n",
    "            if advanced_pinch_detection(handslms, click_threshold):\n",
    "                current_time = time.time()\n",
    "                if current_time - last_click > debounce_time:\n",
    "                    pyautogui.click()\n",
    "                    last_click = current_time\n",
    "                    print(f\"Click detected! Distance: {norm_distance:.3f}\")\n",
    "            \n",
    "            # Visual feedback circles at finger tips\n",
    "            if is_pinching(handslms, click_threshold):\n",
    "                # Red circles when pinching\n",
    "                cv2.circle(frame, (thumb_x, thumb_y), 12, (0, 0, 255), -1)\n",
    "                cv2.circle(frame, (index_x_frame, index_y_frame), 12, (0, 0, 255), -1)\n",
    "                # Outer rings for emphasis\n",
    "                cv2.circle(frame, (thumb_x, thumb_y), 18, (0, 0, 255), 2)\n",
    "                cv2.circle(frame, (index_x_frame, index_y_frame), 18, (0, 0, 255), 2)\n",
    "            else:\n",
    "                # Blue circles normally\n",
    "                cv2.circle(frame, (thumb_x, thumb_y), 8, (255, 0, 0), -1)\n",
    "                cv2.circle(frame, (index_x_frame, index_y_frame), 8, (255, 0, 0), -1)\n",
    "            \n",
    "            # Draw hand landmarks\n",
    "            mpDraw.draw_landmarks(frame, handslms, mpHands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # Display normalized distance info\n",
    "            cv2.putText(frame, f\"Norm Distance: {norm_distance:.3f}\", (10, 30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "            cv2.putText(frame, f\"Threshold: {click_threshold}\", (10, 60), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "    \n",
    "    cv2.imshow('Hand Tracking with Dynamic Line', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Click detected!\n",
      "Click detected!\n",
      "Click detected!\n",
      "Click detected!\n",
      "Click detected!\n",
      "Click detected!\n",
      "Click detected!\n",
      "Click detected!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m debounce_time = \u001b[32m0.5\u001b[39m     \u001b[38;5;66;03m# Minimum time between clicks\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     ret, frame = \u001b[43mcap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[32m     10\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Improved Clicking with Better Pinch Detection\n",
    "cap = cv2.VideoCapture(0)\n",
    "last_click = 0\n",
    "click_threshold = 0.04  # Normalized distance threshold\n",
    "debounce_time = 0.5     # Minimum time between clicks\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    frame = cv2.flip(frame, 1)\n",
    "    x, y, c = frame.shape\n",
    "    \n",
    "    # Convert to RGB for MediaPipe\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb_frame)\n",
    "    \n",
    "    if result.multi_hand_landmarks:\n",
    "        for handslms in result.multi_hand_landmarks:\n",
    "            # Move cursor to index finger tip\n",
    "            index_x = int(handslms.landmark[8].x * s[0])\n",
    "            index_y = int(handslms.landmark[8].y * s[1])\n",
    "            pyautogui.moveTo(index_x, index_y, _pause=False)\n",
    "            \n",
    "            # Advanced pinch detection\n",
    "            if advanced_pinch_detection(handslms, click_threshold):\n",
    "                current_time = time.time()\n",
    "                if current_time - last_click > debounce_time:\n",
    "                    pyautogui.click()\n",
    "                    last_click = current_time\n",
    "                    print(\"Click detected!\")\n",
    "            \n",
    "            # Visual feedback - draw circle when pinching\n",
    "            thumb_x = int(handslms.landmark[4].x * x)\n",
    "            thumb_y = int(handslms.landmark[4].y * y)\n",
    "            index_x_frame = int(handslms.landmark[8].x * x)\n",
    "            index_y_frame = int(handslms.landmark[8].y * y)\n",
    "            \n",
    "            # Draw connection line between thumb and index\n",
    "            cv2.line(frame, (thumb_x, thumb_y), (index_x_frame, index_y_frame), (0, 255, 0), 2)\n",
    "            \n",
    "            # Change color based on pinch state\n",
    "            if is_pinching(handslms, click_threshold):\n",
    "                cv2.circle(frame, (thumb_x, thumb_y), 10, (0, 0, 255), -1)  # Red when pinching\n",
    "                cv2.circle(frame, (index_x_frame, index_y_frame), 10, (0, 0, 255), -1)\n",
    "            else:\n",
    "                cv2.circle(frame, (thumb_x, thumb_y), 8, (255, 0, 0), -1)  # Blue normally\n",
    "                cv2.circle(frame, (index_x_frame, index_y_frame), 8, (255, 0, 0), -1)\n",
    "            \n",
    "            mpDraw.draw_landmarks(frame, handslms, mpHands.HAND_CONNECTIONS)\n",
    "    \n",
    "    cv2.imshow('Output', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)# Alternative Gesture-Based Clicking Methods\n",
    "\n",
    "def finger_gun_click(handslms):\n",
    "    \"\"\"\n",
    "    Detect 'finger gun' gesture (index extended, others folded)\n",
    "    \"\"\"\n",
    "    # Check if index finger is extended and middle finger is folded\n",
    "    index_tip = handslms.landmark[8]\n",
    "    index_pip = handslms.landmark[6]\n",
    "    middle_tip = handslms.landmark[12]\n",
    "    middle_pip = handslms.landmark[10]\n",
    "    \n",
    "    index_extended = index_tip.y < index_pip.y\n",
    "    middle_folded = middle_tip.y > middle_pip.y\n",
    "    \n",
    "    return index_extended and middle_folded\n",
    "\n",
    "def peace_sign_click(handslms):\n",
    "    \"\"\"\n",
    "    Detect peace sign (index and middle extended, others folded)\n",
    "    \"\"\"\n",
    "    index_tip = handslms.landmark[8]\n",
    "    index_pip = handslms.landmark[6]\n",
    "    middle_tip = handslms.landmark[12]\n",
    "    middle_pip = handslms.landmark[10]\n",
    "    ring_tip = handslms.landmark[16]\n",
    "    ring_pip = handslms.landmark[14]\n",
    "    \n",
    "    index_extended = index_tip.y < index_pip.y\n",
    "    middle_extended = middle_tip.y < middle_pip.y\n",
    "    ring_folded = ring_tip.y > ring_pip.y\n",
    "    \n",
    "    return index_extended and middle_extended and ring_folded\n",
    "\n",
    "def thumb_up_click(handslms):\n",
    "    \"\"\"\n",
    "    Detect thumbs up gesture\n",
    "    \"\"\"\n",
    "    thumb_tip = handslms.landmark[4]\n",
    "    thumb_mcp = handslms.landmark[2]\n",
    "    index_tip = handslms.landmark[8]\n",
    "    index_pip = handslms.landmark[6]\n",
    "    \n",
    "    # Thumb extended upward, index folded\n",
    "    thumb_up = thumb_tip.y < thumb_mcp.y\n",
    "    index_folded = index_tip.y > index_pip.y\n",
    "    \n",
    "    return thumb_up and index_folded\n",
    "\n",
    "print(\"Alternative gesture detection functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_dict(dict):\n",
    "    result = 0\n",
    "    for key in dict:\n",
    "        result += dict[key]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding Finger Counter\n",
    "\n",
    "# initialize web cam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\n",
    "    # capture webcame frame and shape (width and height)\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # flip frame for mirror effect\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    h, w, c = frame.shape\n",
    "\n",
    "    # convert to rgb for mediapipe\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # have mediapipe hands predict hand landmarks\n",
    "    result = hands.process(rgb_frame)\n",
    "\n",
    "    # dict of fingers\n",
    "    fingers = {\"4\": 0, \"8\": 0, \"12\": 0, \"16\": 0, \"20\": 0}\n",
    "\n",
    "\n",
    "    # iterate through the predicted landmarks adjusting them to the window, and\n",
    "    # and outputting them to the opencv window\n",
    "    if result.multi_hand_landmarks:\n",
    "        for handslms in result.multi_hand_landmarks:\n",
    "\n",
    "            # thumb coordinates\n",
    "\n",
    "            # a note here is we have to scale coordinates relative to the frame (height, width)\n",
    "            x4, y4 = int(handslms.landmark[4].x * w), int(handslms.landmark[4].y * h)\n",
    "            x2, y2 = int(handslms.landmark[2].x * w), int(handslms.landmark[2].y * h)\n",
    "\n",
    "            # check if thumb is up\n",
    "            if x4 > x2:\n",
    "                fingers['4'] = 1\n",
    "            \n",
    "            # index finger coordinates\n",
    "            x8, y8 = int(handslms.landmark[8].x * w), int(handslms.landmark[8].y * h)\n",
    "            x6, y6 = int(handslms.landmark[6].x * w), int(handslms.landmark[6].y * h)\n",
    "\n",
    "            if y8 < y6:\n",
    "                fingers['8'] = 1\n",
    "            \n",
    "            # middle finger\n",
    "            x12, y12 = int(handslms.landmark[12].x * w), int(handslms.landmark[12].y * h)\n",
    "            x10, y10 = int(handslms.landmark[10].x * w), int(handslms.landmark[10].y * h)\n",
    "\n",
    "            if y12 < y10:\n",
    "                fingers['12'] = 1\n",
    "            \n",
    "            # ring finger\n",
    "            x16, y16 = int(handslms.landmark[16].x * w), int(handslms.landmark[16].y * h)\n",
    "            x14, y14 = int(handslms.landmark[14].x * w), int(handslms.landmark[14].y * h)\n",
    "\n",
    "            if y16 < y14:\n",
    "                fingers['16'] = 1\n",
    "            \n",
    "            # pinky finger\n",
    "            x20, y20 = int(handslms.landmark[20].x * w), int(handslms.landmark[20].y * h)\n",
    "            x18, y18 = int(handslms.landmark[18].x * w), int(handslms.landmark[18].y * h)\n",
    "        \n",
    "            if y20 < y18:\n",
    "                fingers['20'] = 1\n",
    "\n",
    "            mpDraw.draw_landmarks(frame, handslms, mpHands.HAND_CONNECTIONS)\n",
    "\n",
    "    # count raised fingers\n",
    "    number_fingers = sum_dict(fingers)\n",
    "\n",
    "    cv2.rectangle(frame, (25, 150), (100, 400), (0, 128, 0), cv2.FILLED)\n",
    "    cv2.putText(frame, str(number_fingers), (35, 300), cv2.FONT_HERSHEY_PLAIN,\n",
    "                3, (0, 71, 71), 2)\n",
    "    \n",
    "    cv2.imshow('Output', frame)\n",
    "    # if q is pressed, program exits\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Article if you have trouble understanding! https://medium.com/@Mert.A/how-to-create-a-finger-counter-with-python-and-mediapipe-cc6c3911ad09"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
